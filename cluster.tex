\documentclass[twoside,a4paper,12pt,english]{inac17}

%INAC2017 SETUP: SET PAGE SIZE AND SET FOR USING graphicx PACKAGE.
\usepackage{graphicx}
\usepackage{babel,varioref,epsfig} %,rotating}
\usepackage{amssymb}
\usepackage[font=bf,center]{caption}
\usepackage{subfigure}

\usepackage{textcomp}             % Para usar marca registrada

\title{PROFESSIONAL CLUSTER MANAGEMENT BY A SMALL SCIENTIFIC TEAM: CHALLENGES, SOLUTIONS
AND PERSPECTIVES}

%INAC2017 SETUP: SPECIFY AUTHOR NAMES, AFFILIATION, ADDRESS AND E-MAIL.

\author{
  \bf{Vitor V. A. Silva, Andr\'e A. C. dos Santos and Renan O. Cunha}\\ \\
  CDTN - Centro de Desenvolvimento da Tecnologia Nuclear\\
  Av. Ant\^onio Carlos 6627 - Campus UFMG\\
  31270-901 - Belo Horizonte, MG\\
  \{vitors, aacs, roc\}@cdtn.br}

\begin{document}

%INAC2017 SETUP: PRINT TITLE
\maketitle

%INAC2017 SETUP: SETUP HEADS FOR PAGES
\pagestyle{myheadings}
\thispagestyle{empty}
\markboth{}{}


%INAC2017 SETUP: SET FIRST PAGE WITH NO PAGE NUMBER
\thispagestyle{empty}

%--------------------------------------------------------------------------------------

\begin{abstract_full_paper}
  The specification, configuration and management of a professional computer cluster are specialized
tasks usually hold by well trained teams, often full-time hired computer scientists. However, in
many situations and for widely different reasons, these very specific technical tasks must
be carried on by no other than the user itself. This is the situation at Centro de Desenvolvimento
da Tecnologia Nuclear - and in many nuclear research and educational centres in developing countries -
where the scientists are the users of the cluster but also the technical
team responsible to keep the system running. This paper presents the process of planning
and installing the whole operating system and scientific software of a professional cluster
aimed to be used in the nuclear engineering field from the point of view of its users.
The drawbacks of lack of expertise and technical skills to
manage such type of technology are opposed to the advantages of freedom to chose the solutions
which best fit to the problems to be solved. The details of selected methods or technologies
chosen for addressing a specific matter are presented together with other possible options, 
offering a broader view of the whole process of cluster's configuration. Specificities
of dealing with closed, restricted and open software, common in the nuclear engineering field,
are also put in perspective. The ideas and solutions presented in this paper can be a
valuable reference to other research teams found in a similar situation:
being scientists and its own technical staff at the same time.
\end{abstract_full_paper}

%--------------------------------------------------------------------------------------

\section{INTRODUCTION}\label{int}

Now days computers are indispensable tools for scientists of any field.
Some areas of research heavily rely on computational power in order to solve
complex problems by use of demanding algorithms, methods and heuristics.
Current methods used for nuclear reactor calculations, both thermal-hydraulics
and neutronics can be more accurate or even only applicable by using
many cores or computers altogether.

A computer cluster consists of set of computers connected to work together. The difference of the
definition between a computer cluster and a computer grid is that usually grids are more
heterogeneous and used to a different set of problems at the same time, while a computer
cluster is aimed to have each node solving the same problem.

To be able to have sets of computers working together and be able to control the utilization of the system,
it is absolutely fundamental to have an operating system capable of offering interconnection, data sharing,
task schedule and user administration. The \textbf{de facto} operating system widely used in these situations
is Linux \cite{Linux}. After choosing a suitable operating system, its mandatory to evaluate the applications to
be used, the level of security to be enforced and, fundamentally, the dynamics of the operation of the system.
These factors will define the suitable solutions for different aspects of the system, ranging from data
sharing among nodes to options to update, backup and access the system.


%--------------------------------------------------------------------------------------

\subsection{Context}

As aforementioned, the cluster presented in this paper is aimed to scientific computing.
Since the expression scientific computing has many meanings, depending on the context, a
clarification is necessary. This cluster will be utilized to run computational fluid dynamics
(CFD) simulations, neutronics simulations - both using deterministic and Monte Carlo \cite{MC}
methods and coupled calculations, involving the two disciplines. It is also natural to predict
its expansion in utilization to other fields depending on its demand, performance and availability.

The users of the system are mainly nuclear and mechanic engineers, scientists and graduate
and undergraduate students. These users have different levels of knowledge of the work
they must carry and also are unevenly skilled as system users. These differences must be taken
in account when planning and choosing the operational characteristics of the system.

From the perspective of the Centro de Desenvolvimento da Tecnologia Nuclear (CDTN), such system
is a fundamental asset. Computer power is, at the very end, power of calculation and any research
team with demand for intensive computing is a potential user and potential client of the cluster.
With this in mind, there is a real potential of increase of research of CDTN related to
scientific computing. An interesting perspective of the way of research can change with the
use of intensive scientific computing - in this case, the extreme computing - is elegantly
presented by Dongarra \cite{Dongarra2017}.

%Teste de todas as citações disponíveis no bibtex: Wu\cite{Wu2016}, Nagaya\cite{Nagaya2015}.

%------------------------------------------------------------------------------

\section{OBJECTIVE}

The objective of this paper is to present the challenges, problems and solutions proposed
to setup a professional computational cluster to carry heavy numerical calculations focused in
neutronics and thermal-hydraulics of nuclear reactors in a
context of a small research team.

%\begin{equation}
%  \nabla\cdot\mathbf{J} + \Sigma_{a} \Phi - s =0 .
%  \label{eq_1}
%\end{equation}

%------------------------------------------------------------------------------

\section{METHODS AND PROCEDURES}

\subsection{Original Hardware and Software}

The so-called cluster system is formed by a set of identical high performance computers -
total number of eight machines - with a master unit with slightly different configuration.
Figure ~\ref{fig:cluster} presents a schematic representation of the system and
its network topology.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
%  \centering\includegraphics[width=8.5cm,height=8.5cm]{images/esquema_cluster_edited_bw.png}
  \centering\includegraphics[scale=0.25]{images/esquema_cluster_edited_bw-no-net.png}
  \caption{Cluster hardware.}
  \label{fig:cluster}
\end{figure}

The \textit{slave} machines are Dell workstations Precision R7910. These machines are equipped
with Dual Intel{\textregistered} Xeon{\textregistered} processors E5-2640 operating at $2.4$GHz with
$32$Gb of RAM each. They have 2x$10$GbE (Gigabit Ethernet) and 2x$1$Gbit network connectors, which
allow the use of the Gigabit network exclusively for intra-cluster communication. Each node
has a $360$Gb solid state disk for storage.

These equipments are also installed with Nvidia{\textregistered} GPU's (Graphic Processing Units) M4000{\textregistered} with $8$Gb CUDA\cite{CUDA} capable.
The GPU's on nodes, despite capable of visualization, are expected to be used as secondary calculation units.
The use of graphic processors for calculations is widely applied in scientific computing and these divides
are also called accelerators \cite{Accelerators}.

The \textit{master} machine is a Dell Precision T7910, with very similar configuration of the slaves. The main
differences rely on the graphics processor unit, equipped with another Nvidia{\textregistered} GPU: a
Quadro{\textregistered} M2000 with $4$Gb of memory. These video card is not aimed for calculations, but for
visualization of data and graphics display. The other different is in the main RAM memory, having the \textit{master}
$256$Gb of RAM. 

The original OS sold with the cluster is an OEM (Original Equipment Manufacturer) Windows 7{\textregistered} \cite{windows7}. This
OS is not supported by the whole set of software expected to be used by the research group users. The chosen
system to replace the original Windows 7 is CentOS (Community Enterprise Operating System)\cite{centos}, which offers a platform to natively run all
software needed by the users.

\subsection{Linux operating system}

The reason for choosing \textit{CentOS} as the Linux flavor for the system is due to its characteristics and the previous
experience of the cluster management team. CentOS is aimed to offer a free, enterprise-class, community-supported computing
functionality. It appeared as a fork of Red Hat Enterprise Linux, which is available only trough paid subscription. The option
for CentOS, in this context, is to have both the strong development for enterprise systems and also its gratuity.

A second reason is to take advantage of previous competence of the installation and user team which already has experience
in using Fedora Linux, a distribution fully compatible with CentOS.

\subsection{Software solutions}

Before choosing a software for perform a desirable task, its mandatory to have a consistent definition of the environment
in which the system will be used. The first constraint is the network environment. It's worth noting that the cluster is
located in the CDTN network and the restrictions and security rules enforced in the network must be followed by the system.

With this is mind, the choice is to have the system - the slaves machines - in one isolated sub-network only accessed by
the master node. The master node is part of both the sub-network and the institute's network. The sole point of access to the
internet for any slave is the master node which, in turn, is connected to the internet using a proxy connection and a
NAT (Network Access Translation). The topology of the cluster network is presented in Figure ~\ref{fig:esquema-cluster}.

\begin{figure}[h] % t forces top and b forces bottom: can be added to h, ex. [ht]
%  \centering\includegraphics[width=8.5cm,height=8.5cm]{images/esquema_cluster_edited_bw.png}
  \centering\includegraphics[scale=0.25]{images/esquema_cluster_edited_bw.png}
  \caption{Cluster showing its network topology.}
  \label{fig:esquema-cluster}
\end{figure}

This topology have impacts on system maintenance. Since the slaves machines cannot access the internet, the
master is configured to hold a mirror of packages installed in all system machines. It is scheduled to make
updates from time to time (once a week in the current configuration) while the slaves are also scheduled to
make their updates after the master. This approach has advantages, like decreasing the network demand since only
one machine downloads the packages from the internet. Other advantage is in the download time, since the slaves
use dedicated $10$GbE connections to the master which has an average download rate of $500$Kbytes/second in
CDTN's network. The drawback is the amount of disk space needed to keep a mirror of CentOS packages locally.
To decrease the space consumption, only packages already installed on the system - master and slaves - are
downloaded. In the case of the installation of a new package, it must be installed in the master. After that,
the scheduled script guarantees its availability for the slaves.

\subsection{File System}

A crucial element of a computer system architecture is its file system. A file system is the set of data structures and functions an operating system
use to manage and keep track of files on disks and partitions. There is a large number of file systems being used by different operating systems\cite{linuxbook}.

However, in a high performance and high demand system like a cluster, there are some desirable features not usually provided for standard computers file systems.
For example, the ability to have many different interconnected computers reading and writing information concurrently and asynchronously. These kind of feature
is provided by a specialized file system, usually called distributed file system. \cite{hal}

\begin{itemize}

\item (Possíveis soluções/soluções encontradas em relação a cada característica apresentada. Por exemplo: Lustre \cite{Lustre} ou Gluster\cite{gluster} para o sistema de arquivos?)

\item (Para cada solução escolhida: Por quê?)
  
\end{itemize}

However, there are more than one way of setup a cluster system with respect
to file system, software execution and user access.

The next paragraphs, describe different option of setup for a Linux cluster with respect to software execution.

\begin{itemize}
\item Direct OS installation
\item Use of dockers
\item SO virtualization
\end{itemize}

\subsection{Making use of graphics processors}

The system under consideration has powerful GPU cards installed in machines
which have no output display. The use of GPU's to perform tasks other than graphics processing
is nowadays widespread in many fields of research \cite{UsoDeGpus}. Modern software is written with
the objective of taking advantage of this ``extra'' processing capabilities and studies of software
engineering on parallelization of sequential algorithms to be applied in GPU's are a significant
nowadays.

In order to make the use of GPU's for general processing less difficult, libraries, compilers and
frameworks were developed. At the beginning these tools were developed and deployed by graphics cards
manufacturers. The pioneer in offering both GPU's cards capable of general use is NVIDIA, which,
without surprise, is also pioneer in making a available a full framework for its GPU programming.
This tool is called CUDA (Compute Unified Device Architecture) and is a parallel computing platform
and application programming interface (API) model.

However, with the increase of the use the cards, developers start to demand some
standardization in order to be able experiment new algorithms in different cards. Today, the
reference framework for development for heterogeneous platforms is \textit{OpenCL} \cite{OpenCL}.

--- Como funciona o OpenCL

The idea for the cluster system, is to provide both CUDA libraries for
software developed for NVIDIA graphic cards and also OpenCL libraries to
be used by software written to make use of the programming genericity
provided by OpenCL. As the time of writing, the libraries were installed
in only one node of the cluster and tests are being envisaged to check
compatibility of both frameworks.

\section{RESULTS AND ANALYSIS}

\begin{itemize}

\item (Situação atual do sistema - no momento de finalizer o paper)
\item (Dificuldades identificadas: Resolvidas (como) e Pendentes.
  
\end{itemize}

%------------------------------------------------------------------------------
\subsubsection{Sub-subsection level and lower: only first character uppercase}

Figures and tables should appear as closely as possible to where they are first cited, e.g. Fig.~\ref{esquema-cluster}, in the text.  Figures are numbered in Arabic numerals, with the caption centered below the figure, in boldface. Double-space before the figure, and after the figure caption.


\newcommand{\cc}{\centering}
\newcommand{\rr}{\raggedright}
\newcommand{\tn}{\tabularnewline}
\renewcommand{\arraystretch}{1.5}
\begin{table}[h]
\caption{Numerical results to the model problem} %title of the table
\centering % centering table
\begin{tabular}{|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\cc Mesh             &\cc 8$\times$8  &\cc 16$\times$16   &\cc 32$\times$32   &\cc 64$\times$64 \tn \hline        
\rr Nodal            &\cc 1.000       &\cc 2.500          &\cc 6.250          &\cc 1.563        \tn \hline
\rr Characteristic   &\cc 1.000       &\cc 2.500          &\cc 6.250          &\cc 1.563        \tn \hline
\end{tabular}
\label{table_1}
\end{table}

When importing figures or any graphical image please verify two things:

\begin{itemize}

\item Any number, text or symbol is in Times font and is not smaller than 10-point after reduction to the actual window in your paper;

\item That it can be translated into PDF.

\end{itemize}



Tables, like Table~\ref{table_1}, are numbered in Arabic numerals, with the caption centered above the table, in {\bf boldface}.  Double-space before and after the table.

%------------------------------------------------------------------------------

\section{CONCLUSIONS AND PERSPECTIVES}

(O que está bom, o que está ruim na atual instalação)
(Trabalhos futuros: pra onde ir (operação), possíveis mudanças)

%Uma cita\c{c}\~{a}o \cite{Henderson17}.


%------------------------------------------------------------------------------



\section*{Acknowledgments}
The authors would like to thank FUJB for financing the cluster acquisition
as part of the project \textit{Desenvolvimento de novos elementos combust\'{i}veis nucleares
  e materiais e pe\c{c}as para combust\'{i}veis nucleares}, agreement FINEP 01.07.0548.00 - Process FUJB 13.867-3.
The authors also thank Dr. Jo\~{a}o Roberto Loureiro de Mattos for the work which made possible the acquisition
of the cluster.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99} %99 é o número máximo que o thebibliography permite. Numero de referencias que aparecerão.

\bibitem{linuxbook} Wirzenius and Lars, \textit{The  Linux System Administrator's Guide}, iUniverse incorporated (2000).

  % author = {Wirzenius, Lars},
% title = {The  Linux System Administrator's Guide},
% year = {2000},
% isbn = {0595137636},
% publisher = {iUniverse, Incorporated},
%} 

\bibitem{gluster} Alex Davies and Alessandro Orsaria, ``Scale out with GlusterFS'', \textit{The Linux Journal}, \textbf{2013}, (2013).
% journal = {Linux J.},
% issue_date = {November 2013},
% volume = {2013},
% number = {235},
% month = nov,
% year = {2013},
% issn = {1075-3583},
% articleno = {1},
% url = {http://dl.acm.org/citation.cfm?id=2555789.2555790},
% acmid = {2555790},
% publisher = {Belltown Media},
% address = {Houston, TX},
%} 
\bibitem{hall} Benjamin Depardon, Ga\"{e}l Le Mahec and Cyril S\'{e}guin, ``Analysis of Six Distributed File Systems'', Research Report, pp. 44, (2013).

%\bibitem{article} B. Author(s), ``Title", \textit{Journal Name in Italic}, \textbf{Volume in Bold}, pp. 34--89, (19xx).

%\bibitem{proceeding} C. D. Author(s), ``Article Title", \textit{Proceeding of Meeting in Italic}, Location, Dates of Meeting, Vol. n, pp. 134--156, (19xx).

%\bibitem{book} E. F. Author. \textit{Book Title in Italic}, Publisher, City \& Country (19xx).

%\bibitem{website} ``Spallation Neutron Source: The next-generation neutron-scattering facility for the United States", \verb#http://www.sns.gov/documentation/sns_brochure.pdf# (2002).

\end{thebibliography}

% ---------------------------------------------------------
% Minha bibliografia usando arquivo externo

%\bibliographystyle{unsrt}
%\bibliography{bibli}


\end{document}
